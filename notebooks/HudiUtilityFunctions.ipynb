{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "friendly-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip --disable-pip-version-check install -q numpy\n",
    "!pip --disable-pip-version-check install -q pandas\n",
    "!pip --disable-pip-version-check install -q pyhive\n",
    "!pip --disable-pip-version-check install -q faker\n",
    "!pip --disable-pip-version-check install -q boto3\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from pyhive import presto as pres\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "import boto3\n",
    "\n",
    "BUCKET = \"presto-workshop-bucket-us-east-2\"\n",
    "SANDBOX_BASE_PATH = f\"s3a://{BUCKET}/sandbox/\"\n",
    "INPUT = f\"s3a://{BUCKET}/input/1gb/\"\n",
    "\n",
    "def listFiles(prefixOrPath: str):\n",
    "    s3 = boto3.client('s3')\n",
    "    if prefixOrPath.startswith(\"s3://\") or prefixOrPath.startswith(\"s3a://\"):\n",
    "        # Extract bucket name and prefix\n",
    "        parts = prefixOrPath.split(\"/\", 3)\n",
    "        bucket_name = parts[2]\n",
    "        prefix = \"\" if len(parts) == 3 else parts[3]\n",
    "        \n",
    "        # List objects in the specified bucket with the given prefix\n",
    "        response = s3.list_objects_v2(\n",
    "            Bucket=bucket_name,\n",
    "            Prefix=prefix\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        response = s3.list_objects_v2(\n",
    "            Bucket=BUCKET,\n",
    "            Prefix=prefixOrPath\n",
    "        )\n",
    "    if 'Contents' in response:\n",
    "        contents = response['Contents']\n",
    "        df = pd.DataFrame(contents)\n",
    "        display(HTML(df.to_html()))\n",
    "    else:\n",
    "        print(\"No files found in the specified path.\")\n",
    "\n",
    "def copy_files_within_bucket(source_bucket, source_prefix, destination_bucket, destination_prefix):\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.list_objects_v2(Bucket=source_bucket, Prefix=source_prefix)\n",
    "    for obj in response.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        copy_source = {'Bucket': source_bucket, 'Key': key}\n",
    "        new_key = key.replace(source_prefix, destination_prefix, 1)\n",
    "        s3.copy_object(CopySource=copy_source, Bucket=destination_bucket, Key=new_key)\n",
    "\n",
    "def dis(df):\n",
    "    start_time = datetime.now()\n",
    "    html_content = df.limit(50).toPandas().to_html()\n",
    "    end_time = datetime.now() \n",
    "    time_difference = end_time - start_time\n",
    "    print(\"Time: \", time_difference)\n",
    "    return display(HTML(html_content))\n",
    "\n",
    "def presto(query):\n",
    "    start_time = datetime.now()\n",
    "    presto_conn = pres.Connection(host=\"presto.hudi-infra.svc.cluster.local\", port=9090)\n",
    "    presto_cursor=presto_conn.cursor()\n",
    "    presto_cursor.execute(query)\n",
    "    rows = presto_cursor.fetchmany(50)\n",
    "    column_names = [desc[0] for desc in presto_cursor.description]\n",
    "    if len(rows) == 0:\n",
    "        print(\"No results\")\n",
    "        return\n",
    "    else:\n",
    "        df2 = pd.DataFrame(rows, columns=column_names)\n",
    "        end_time = datetime.now() \n",
    "        time_difference = end_time - start_time\n",
    "        print(\"Time: \", time_difference)\n",
    "        return display(HTML(df2.to_html()))\n",
    "    \n",
    "def presto_with_metadata_enabled(query):\n",
    "    start_time = datetime.now()\n",
    "    presto_conn = pres.Connection(host=\"presto.hudi-infra.svc.cluster.local\", port=9090)\n",
    "    presto_cursor=presto_conn.cursor()\n",
    "    key = \"hive.hudi_metadata_enabled\"\n",
    "    value = \"true\"\n",
    "    q = f\"SET session {key} = {value}\"\n",
    "    presto_cursor.execute(q)\n",
    "    presto_cursor.fetchall()\n",
    "    presto_cursor.execute(query)\n",
    "    rows = presto_cursor.fetchmany(50)\n",
    "    column_names = [desc[0] for desc in presto_cursor.description]\n",
    "    if len(rows) == 0:\n",
    "        print(\"No results\")\n",
    "        return\n",
    "    else:\n",
    "        df2 = pd.DataFrame(rows, columns=column_names)\n",
    "        end_time = datetime.now() \n",
    "        time_difference = end_time - start_time\n",
    "        print(\"Time: \", time_difference)\n",
    "        return display(HTML(df2.to_html()))\n",
    "\n",
    "\n",
    "ZOOKEEPER_LOCK_CONFIGS = {\n",
    "                    \"hoodie.cleaner.policy.failed.writes\" : \"LAZY\",\n",
    "                    \"hoodie.write.concurrency.mode\" : \"optimistic_concurrency_control\",\n",
    "                    \"hoodie.write.lock.provider\" : \"org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider\",\n",
    "                    \"hoodie.write.lock.zookeeper.url\" : \"zk-cs.hudi-infra.svc.cluster.local\",\n",
    "                    \"hoodie.write.lock.zookeeper.port\" : \"2181\",\n",
    "                    \"hoodie.write.lock.zookeeper.base_path\" : \"/test\"\n",
    "}\n",
    "\n",
    "DISABLE_TIMELINE_CONFIGS = {\n",
    "    \"hoodie.write.markers.type\" : 'direct',\n",
    "    'hoodie.embed.timeline.server' : 'false',\n",
    "}\n",
    "\n",
    "\n",
    "def get_spark_session(app_name = \"Jupyter Job\", hudi_version=\"0.14.1\", num_executors=1 ):\n",
    "    conf=swan_spark_conf\n",
    "    config = {\n",
    "    \"spark.kubernetes.namespace\": \"spark\",\n",
    "    \"spark.kubernetes.container.image\": \"itayb/spark:3.1.1-hadoop-3.2.0-aws\",\n",
    "    \"spark.executor.instances\": num_executors,\n",
    "    \"spark.executor.memory\": \"48g\",\n",
    "    \"spark.executor.cores\": \"8\",\n",
    "    \"spark.driver.blockManager.port\": \"7777\",\n",
    "    \"spark.driver.port\": \"2222\",\n",
    "    \"spark.driver.host\": \"jupyter.spark.svc.cluster.local\",\n",
    "    \"spark.driver.bindAddress\": \"0.0.0.0\",\n",
    "    \"spark.jars.packages\" : f\"org.apache.hudi:hudi-spark3.1-bundle_2.12:{hudi_version}\",\n",
    "    \"spark.serializer\" : \"org.apache.spark.serializer.KryoSerializer\",\n",
    "    \"spark.sql.extensions\" : \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\",\n",
    "    \"spark.kryo.registrator\" : \"org.apache.spark.HoodieSparkKryoRegistrar\",\n",
    "    \"spark.hadoop.hive.metastore.uris\" : \"thrift://metastore123.hudi-infra.svc.cluster.local:9083\",\n",
    "    }\n",
    "    conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local\")\n",
    "    for key, value in config.items():\n",
    "        conf.set(key, value)\n",
    "    spark=SparkSession.builder.appName(app_name).config(conf=conf).enableHiveSupport().getOrCreate()\n",
    "    print(\"Spark Session started at port : \" + spark.sparkContext.uiWebUrl.split(\":\")[-1])\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-designer",
   "metadata": {},
   "source": [
    "# Hudi Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QUICKSTART_BASE_PATH = SANDBOX_BASE_PATH + \"/quickstart_sandbox/\"\n",
    "\n",
    "def quickstart(spark, tableName = \"hudi_table\", extraConfigs = {}, partitioned = True, pkless = False, is_mor = False):\n",
    "    basePath = QUICKSTART_BASE_PATH + tableName\n",
    "    columns = [\"ts\",\"uuid\",\"rider\",\"driver\",\"fare\",\"city\"]\n",
    "    data =[(1695159649087,\"334e26e9-8355-45cc-97c6-c31daf0df330\",\"rider-A\",\"driver-K\",19.10,\"san_francisco\"),\n",
    "       (1695091554788,\"e96c4396-3fad-413a-a942-4cb36106d721\",\"rider-C\",\"driver-M\",27.70 ,\"san_francisco\"),\n",
    "       (1695046462179,\"9909a8b1-2d15-4d3d-8ec9-efc48c536a00\",\"rider-D\",\"driver-L\",33.90 ,\"san_francisco\"),\n",
    "       (1695516137016,\"e3cf430c-889d-4015-bc98-59bdce1e530c\",\"rider-F\",\"driver-P\",34.15,\"sao_paulo\"),\n",
    "       (1695115999911,\"c8abbe79-8d89-47ea-b4ce-4d224bae5bfa\",\"rider-J\",\"driver-T\",17.85,\"chennai\")]\n",
    "    inserts = spark.createDataFrame(data).toDF(*columns)\n",
    "\n",
    "    hudi_options = {\n",
    "        'hoodie.table.name': tableName,\n",
    "        'hoodie.datasource.write.recordkey.field': 'uuid'\n",
    "    }\n",
    "    \n",
    "    hudi_options.update(DISABLE_TIMELINE_CONFIGS)\n",
    "    \n",
    "    if(partitioned):\n",
    "        hudi_options.update({'hoodie.datasource.write.partitionpath.field': 'city'})\n",
    "    if(not pkless):\n",
    "        hudi_options.update({\n",
    "            'hoodie.datasource.write.recordkey.field': 'uuid',\n",
    "            'hoodie.datasource.write.precombine.field': 'ts'\n",
    "        })\n",
    "        \n",
    "    hudi_options.update(extraConfigs)\n",
    "\n",
    "    inserts.write.format(\"hudi\"). \\\n",
    "        options(**hudi_options). \\\n",
    "        mode(\"overwrite\"). \\\n",
    "        save(basePath)\n",
    "    print(\"Data generated successfully\")\n",
    "    tripsDF = spark.read.format(\"hudi\").load(basePath)\n",
    "    tripsDF.createOrReplaceTempView(\"trips_table\")\n",
    "\n",
    "    spark.sql(\"SELECT uuid, fare, ts, rider, driver, city FROM  trips_table WHERE fare > 20.0\").show()\n",
    "    spark.sql(\"SELECT _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare FROM trips_table\").show()\n",
    "    \n",
    "    updatesDf = spark.read.format(\"hudi\").load(basePath).filter(\"rider == 'rider-D'\").withColumn(\"fare\",col(\"fare\")*10)\n",
    "\n",
    "    updatesDf.write.format(\"hudi\"). \\\n",
    "      options(**hudi_options). \\\n",
    "      mode(\"append\"). \\\n",
    "      save(basePath)\n",
    "    dis(spark.read.format(\"hudi\").load(basePath))\n",
    "    \n",
    "    print(\"Data upserted successfully\")\n",
    "    deletesDF = spark.read.format(\"hudi\").load(basePath).filter(\"rider == 'rider-F'\")\n",
    "\n",
    "    hudi_hard_delete_options = {\n",
    "      'hoodie.table.name': tableName,\n",
    "      'hoodie.datasource.write.partitionpath.field': 'city',\n",
    "      'hoodie.datasource.write.operation': 'delete',\n",
    "    }\n",
    "    \n",
    "    hudi_options.update(hudi_hard_delete_options)\n",
    "\n",
    "    deletesDF.write.format(\"hudi\"). \\\n",
    "        options(**hudi_options). \\\n",
    "        mode(\"append\"). \\\n",
    "        save(basePath)\n",
    "    print(\"Data deleted successfully\")\n",
    "    \n",
    "    dis(spark.read.format(\"hudi\").load(basePath))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
