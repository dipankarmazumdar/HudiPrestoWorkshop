{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "second-photographer",
   "metadata": {},
   "source": [
    "# Please enter a `USER_NUMBER` below:\n",
    "## Use a combination of the first 3 letters of your name & number assigned to you as shown in the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run HudiUtilityFunctions.ipynb\n",
    "USER_NUMBER = \"dip7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = get_spark_session(\"Hudi Presto workshop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-wireless",
   "metadata": {},
   "source": [
    "## Dataset File Location (in Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "listFiles(INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-memory",
   "metadata": {},
   "source": [
    "# Part 1: Create Hudi Tables, Read using Presto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-hydrogen",
   "metadata": {},
   "source": [
    "### First, we will create a Hudi Copy-on-Write Table using a 1 GB dataset (Brooklyn TPC-DS) that is stored in a S3 bucket. We will write the Hudi table in a S3 bucket.\n",
    "### Few of the important parameters: \n",
    "- `hoodie.table.keygenerator.class` : extracts a key out of incoming records. Both record key and partition paths comprise more than 1 field\n",
    "- `hoodie.datasource.write.recordkey.field` :  Value to be used as the recordKey component of HoodieKey\n",
    "- `hoodie.datasource.hive_sync.enable` : When set to `True`, syncs the table to Hive metastore\n",
    "- `hoodie.datasource.write.precombine.field` : Field used in preCombining before actual write. Helps in choosing latest version of a record when multiple versions with the same record key exist\n",
    "\n",
    "### `mode(Overwrite)` : overwrites and recreates the table if it already exists.\n",
    "### Here we are using the default write operation : `upsert`. If you have a workload without updates, you can also issue `insert` or `bulk_insert` operations which could be faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-intake",
   "metadata": {},
   "source": [
    "### Copy-on-Write (CoW): Re-Writes a new Parquet file version for updates (synchronous merge during write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_NAME_1 = 'presto_hudi_demo_cow_' + USER_NUMBER\n",
    "\n",
    "df_cow = spark.read.parquet(INPUT)\n",
    "\n",
    "PATH = SANDBOX_BASE_PATH + USER_NUMBER + \"/output/hudi/1gb/\" + TABLE_NAME_1\n",
    "\n",
    "hudi_options = {\n",
    "        'hoodie.table.name': TABLE_NAME_1,\n",
    "        'hoodie.table.keygenerator.class' : \"org.apache.hudi.keygen.ComplexKeyGenerator\",\n",
    "        'hoodie.datasource.write.hive_style_partitioning' : \"true\",\n",
    "        'hoodie.datasource.write.recordkey.field' : \"ss_item_sk,ss_ticket_number\",\n",
    "        'hoodie.datasource.hive_sync.enable' : 'true',\n",
    "        'hoodie.datasource.hive_sync.mode': \"hms\",\n",
    "        'hoodie.datasource.write.precombine.field' : \"ss_sold_time_sk\",\n",
    "        'hoodie.parquet.max.file.size' : '12582912',\n",
    "        'hoodie.parquet.small.file.limit': '10485760'    \n",
    "    }\n",
    "# hudi_options.update(ZOOKEEPER_LOCK_CONFIGS)\n",
    "hudi_options.update(DISABLE_TIMELINE_CONFIGS)\n",
    "print(hudi_options)\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + TABLE_NAME_1)\n",
    "df_cow.write.format(\"hudi\").mode(\"overwrite\").options(**hudi_options).mode(\"overwrite\").save(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-produce",
   "metadata": {},
   "source": [
    "### Check the list of Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "listFiles(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-stationery",
   "metadata": {},
   "source": [
    "# Run Presto Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-retention",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "presto(f\"SELECT count(*) FROM {TABLE_NAME_1} where ca_location_type = 'condo'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-bones",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "presto(f\"SELECT AVG(ss_quantity) FROM {TABLE_NAME_1} where ca_location_type = 'condo'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-germany",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "presto(f\"\"\"\n",
    "SELECT SUM(ss_sales_price), ca_zip FROM {TABLE_NAME_1} \n",
    "GROUP BY ca_zip HAVING COUNT(DISTINCT ca_location_type) = 3\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-device",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "presto(f\"\"\"\n",
    "SELECT ca_location_type, ca_zip, SUM(ss_sales_price) FROM {TABLE_NAME_1} \n",
    "GROUP BY ROLLUP (ca_zip, ca_location_type) HAVING ca_location_type IS NOT NULL ORDER BY ca_zip\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-separation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "presto(f\"\"\"\n",
    "SELECT COUNT(*), ca_city, ca_county, avg(ss_sales_price) from {TABLE_NAME_1} \n",
    "GROUP BY ca_location_type, ca_city, ca_county HAVING ca_location_type = 'apartment' ORDER BY ca_county\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-castle",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-dover",
   "metadata": {},
   "source": [
    "# Create MoR Table\n",
    "\n",
    "### Merge-on-Read (MoR): Stores data using a combination of Parquet + row-based log files. Updates are logged to log files & later compacted to produce new versions of columnar files synchronously or asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-syntax",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TABLE_NAME_2 = 'presto_hudi_demo_mor_' + USER_NUMBER\n",
    "\n",
    "df_mor = spark.read.parquet(INPUT)\n",
    "\n",
    "PATH = SANDBOX_BASE_PATH + USER_NUMBER + \"/output/hudi/1gb/\" + TABLE_NAME_2\n",
    "\n",
    "hudi_options = {\n",
    "        'hoodie.table.name': TABLE_NAME_2,\n",
    "        'hoodie.datasource.write.table.type': 'MERGE_ON_READ',\n",
    "        'hoodie.table.keygenerator.class' : \"org.apache.hudi.keygen.ComplexKeyGenerator\",\n",
    "        'hoodie.datasource.write.hive_style_partitioning' : \"true\",\n",
    "        'hoodie.datasource.write.recordkey.field' : \"ss_item_sk,ss_ticket_number\",\n",
    "        'hoodie.datasource.hive_sync.enable' : 'true',\n",
    "        'hoodie.datasource.hive_sync.mode': \"hms\",\n",
    "        'hoodie.datasource.write.precombine.field' : \"ss_sold_time_sk\",\n",
    "        'hoodie.parquet.max.file.size' : '12582912',\n",
    "        'hoodie.parquet.small.file.limit': '10485760'    \n",
    "    }\n",
    "hudi_options.update(ZOOKEEPER_LOCK_CONFIGS)\n",
    "hudi_options.update(DISABLE_TIMELINE_CONFIGS)\n",
    "print(hudi_options)\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + TABLE_NAME_2)\n",
    "df_mor.write.format(\"hudi\").mode(\"overwrite\").options(**hudi_options).mode(\"overwrite\").save(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-birmingham",
   "metadata": {},
   "source": [
    "## 1. Run Real Time Queries using Presto\n",
    "### Exposes near-real time data by merging the base and log files of the latest file slice on-the-fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-cheat",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "presto(f\"SELECT count(*) FROM {TABLE_NAME_2}_rt where ca_location_type = 'condo'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-retailer",
   "metadata": {},
   "source": [
    "## 2. Run Read Optimized Queries using Presto\n",
    "### Exposes only the base/columnar files in the latest file slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "presto(f\"SELECT count(*) FROM {TABLE_NAME_2}_ro where ca_location_type = 'condo'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-contact",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-truth",
   "metadata": {},
   "source": [
    " # Part 2: Run Clustering Service using Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-xerox",
   "metadata": {},
   "source": [
    "### A common challenge in analytical workloads is managing the discrepancy between arrival and event times. We want to write data as it arrives, which leads to data being written based on 'arrival time'. However, this means  data can be scattered across different files.\n",
    "\n",
    "### So, query engines might have to read the entire set of data files. Even with predicate pushdown, we can end up with scanning a lot of data.\n",
    "\n",
    "### Clustering is a data layout optimization technique. It is extremely important to organize & lay out your data in storage in an optimized way.\n",
    "\n",
    "\n",
    "### [ `Clustering`](https://hudi.apache.org/docs/next/clustering) in Hudi allows to deal with this.\n",
    "\n",
    "### 2 Steps to cluster: \n",
    "\n",
    "- `Schedule`: create a clustering plan\n",
    "- `Execute`: process the plan to create new files & replace old files\n",
    "\n",
    "\n",
    "### Spark SQL Procedure: `run_clustering()`\n",
    "\n",
    "### Parameters:\n",
    "- `hoodie.clustering.async.max.commits` : Config to control frequency of async clustering\n",
    "- `hoodie.clustering.plan.strategy.sort.columns` : Columns to sort the data by when clustering\n",
    "- `hoodie.clustering.plan.strategy.small.file.limit` : Files smaller than the size specified here are candidates for clustering\n",
    "- `hoodie.write.concurrency.mode` : Concurrency modes for write operations (OCC- Multiple writers can operate on the table with lazy conflict resolution using locks)\n",
    "\n",
    "**scheduleandexecute: Make a clustering plan first and execute that plan immediately**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-pointer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CALL run_clustering(\n",
    "        table => '{TABLE_NAME_1}',\n",
    "        op => 'scheduleandexecute',\n",
    "        options => 'hoodie.clustering.async.max.commits=4,\n",
    "                    hoodie.clustering.plan.strategy.sort.columns=ca_location_type,\n",
    "                    hoodie.clustering.plan.strategy.small.file.limit=629145600,\n",
    "                    hoodie.clustering.plan.strategy.target.file.max.bytes=1073741824,\n",
    "                    hoodie.write.markers.type=direct,\n",
    "                    hoodie.embed.timeline.server=false,\n",
    "                    hoodie.cleaner.policy.failed.writes=LAZY,\n",
    "                    hoodie.write.concurrency.mode=optimistic_concurrency_control,\n",
    "                    hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider,\n",
    "                    hoodie.write.lock.zookeeper.url=zk-cs.hudi-infra.svc.cluster.local,\n",
    "                    hoodie.write.lock.zookeeper.port=2181,\n",
    "                    hoodie.write.lock.zookeeper.base_path=/test'\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-stable",
   "metadata": {},
   "source": [
    "# Run Presto Queries after Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-bones",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "presto(f\"SELECT count(*) FROM {TABLE_NAME_1} where ca_location_type = 'condo'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-burden",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "presto(f\"SELECT AVG(ss_quantity) FROM {TABLE_NAME_1} where ca_location_type = 'condo'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-trail",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "presto(f\"\"\"\n",
    "SELECT SUM(ss_sales_price), ca_zip FROM {TABLE_NAME_1} \n",
    "GROUP BY ca_zip HAVING COUNT(DISTINCT ca_location_type) = 3\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-great",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "presto(f\"\"\"\n",
    "SELECT ca_location_type, ca_zip, SUM(ss_sales_price) FROM {TABLE_NAME_1} \n",
    "GROUP BY ROLLUP (ca_zip, ca_location_type) HAVING ca_location_type IS NOT NULL ORDER BY ca_zip\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-merchandise",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "presto(f\"\"\"\n",
    "SELECT COUNT(*), ca_city, ca_county, avg(ss_sales_price) from {TABLE_NAME_1} \n",
    "GROUP BY ca_location_type, ca_city, ca_county HAVING ca_location_type = 'apartment' ORDER BY ca_county\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-findings",
   "metadata": {},
   "source": [
    "### We can see that even at a scale of 1 GB, there are differences in query execution time. Imagine in a real-world where we have TB or PB scale datasets.\n",
    "\n",
    "### PrestoUI provides clarity on the number of records scanned by the query engine. Let's check the Presto UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-switzerland",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-banking",
   "metadata": {},
   "source": [
    "# Part 3: Using Hudi's Metadata Table with Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-section",
   "metadata": {},
   "source": [
    "### To get the current state of a table, costly files list operations on S3 has to be performed while reading or writing a table. This can be avoided by enabling and using Hudi metadata table indices feature.\n",
    "\n",
    "- `key = \"hive.hudi_metadata_enabled\"`\n",
    "- `key = \"hudi.hudi_metadata_table_enabled\"`\n",
    "\n",
    "### This will creates Files Index under the table path's .hoodie/metadata/files prefix & avoids costly file listings during read or writes by reading file list from the prefix instead of performing a whole table prefix file scan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-corporation",
   "metadata": {},
   "source": [
    "# Run Presto Query with Metadata table enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "presto_with_metadata_enabled(f\"SELECT count(*) FROM {TABLE_NAME_1} where ca_location_type = 'condo'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "presto_with_metadata_enabled(f\"SELECT AVG(ss_quantity) FROM {TABLE_NAME_1} where ca_location_type = 'condo'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-tunnel",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-bunny",
   "metadata": {},
   "source": [
    "# Additional Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-kruger",
   "metadata": {},
   "source": [
    "## Perform Record-Level Writes: UPDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-procedure",
   "metadata": {},
   "source": [
    "### Set the Timeline server to `True` to rely on the timeline server for metadata transactions and to ensure data consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SET hoodie.embed.timeline.server = 'true'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * from {TABLE_NAME_1} LIMIT 5\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-queen",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT ss_list_price, * FROM {TABLE_NAME_1} WHERE ss_item_sk = 17587 AND ss_ticket_number = 129234\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-private",
   "metadata": {},
   "source": [
    "## Run a record-level `UPDATE` on the Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'''\n",
    "UPDATE {TABLE_NAME_1}\n",
    "SET ss_list_price = 150.0\n",
    "WHERE ss_item_sk = 17587 AND ss_ticket_number = 129234''');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-nightmare",
   "metadata": {},
   "source": [
    "### Check if the record changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-librarian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT ss_list_price, * FROM {TABLE_NAME_1} WHERE ss_item_sk = 17587 AND ss_ticket_number = 129234\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-monkey",
   "metadata": {},
   "source": [
    "## Clustering Strategies: Z-order/Hilbert (Multi-dimensional clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CALL run_clustering(\n",
    "        table => '{TABLE_NAME_2}_rt',\n",
    "        op => 'scheduleandexecute',\n",
    "        order_strategy => 'z-order',\n",
    "        options => 'hoodie.clustering.async.max.commits=4,\n",
    "                    hoodie.clustering.plan.strategy.sort.columns=ca_location_type,c_birth_country\n",
    "                    hoodie.clustering.plan.strategy.small.file.limit=629145600,\n",
    "                    hoodie.clustering.plan.strategy.target.file.max.bytes=1073741824,\n",
    "                    hoodie.write.markers.type=direct,\n",
    "                    hoodie.embed.timeline.server=false,\n",
    "                    hoodie.cleaner.policy.failed.writes=LAZY,\n",
    "                    hoodie.write.concurrency.mode=optimistic_concurrency_control,\n",
    "                    hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider,\n",
    "                    hoodie.write.lock.zookeeper.url=zk-cs.hudi-infra.svc.cluster.local,\n",
    "                    hoodie.write.lock.zookeeper.port=2181,\n",
    "                    hoodie.write.lock.zookeeper.base_path=/test'\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "presto(f\"SELECT count(*) FROM {TABLE_NAME_2}_rt where ca_location_type = 'condo'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "presto(f\"SELECT count(*) FROM {TABLE_NAME_2}_rt where ca_location_type = 'condo' and c_birth_country = 'UNITED STATES'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-choir",
   "metadata": {},
   "source": [
    "# Notebook Ends"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
